{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "toc_visible": true,
      "authorship_tag": "ABX9TyNHUL0EFAUM+0M9jC977ccC",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/lalonardoni/treinamento-ia-gen/blob/main/TreinamentoIAGen.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução à IA Generativa e ao LangChain"
      ],
      "metadata": {
        "id": "6NU7doJp4R3X"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "A IA Generativa é uma das inovações mais excitantes na tecnologia de Inteligência Artificial. Ela não apenas analisa dados, mas também gera conteúdo novo e original com base nos padrões que aprendeu. Isso pode incluir desde a geração de texto, imagens, até música e muito mais. O que a diferencia de outros tipos de IA é a sua capacidade de criar a partir de prompts simples, trazendo soluções inovadoras e adaptáveis para os mais variados cenários."
      ],
      "metadata": {
        "id": "kd-A1fea4t9G"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução ao LangChain"
      ],
      "metadata": {
        "id": "__ardW_M5q7M"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O LangChain é uma biblioteca que nos ajuda a trabalhar diretamente com modelos de linguagem (ou LLMs, Large Language Models) de forma eficiente. Ele foi projetado para facilitar a integração desses modelos em aplicações mais complexas, como pipelines de IA que envolvem conversação, análise de texto e até automação de tarefas. O grande diferencial do LangChain é sua capacidade de conectar LLMs a ferramentas externas e outras fontes de dados, o que possibilita a criação de agentes de IA mais poderosos e contextualmente ricos."
      ],
      "metadata": {
        "id": "5G9_xrgk5xzx"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "\n",
        "**LangChain**\n",
        "\n",
        "Existem dois métodos para trabalhar com o LangChain: como uma cadeia sequencial de comandos predefinidos ou usando agentes LangChain. Cada abordagem é diferente na maneira como lida com ferramentas e orquestração. Uma cadeia segue um fluxo de trabalho linear predefinido, enquanto um agente atua como um coordenador que pode tomar decisões mais dinâmicas (não lineares).\n",
        "\n",
        "*   Chains\n",
        "\n",
        "Uma sequência de etapas que pode incluir chamadas para um llm, agente, ferramenta, fonte de dados externa, código de procedimento e muito mais. As cadeias podem se ramificar, o que significa que uma única cadeia se divide em vários caminhos com base em condições lógicas.\n",
        "*   Agents or Language Models\n",
        "\n",
        "Um modelo de linguagem tem a capacidade de gerar respostas em linguagem natural. Mas o Agente usa um modelo de linguagem mais recursos adicionais para raciocinar, chamar ferramentas e repetir o processo de chamar ferramentas caso haja alguma falha\n",
        "*   Tools\n",
        "\n",
        "Funções baseadas em código que podem ser chamadas na cadeia ou invocadas por um agente para interagir com sistemas externos.\n",
        "*   Prompts\n",
        "\n",
        "Isso pode incluir um prompt do sistema que instrui o modelo sobre como concluir uma tarefa e quais ferramentas estão disponíveis, informações injetadas de fontes de dados externas que forneceram mais contexto ao modelo e o prompt ou tarefa do usuário para o modelo concluir."
      ],
      "metadata": {
        "id": "BoBeOjPZ9xGl"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**LangGraph**\n",
        "*   Graphs\n",
        "*   Nodes\n",
        "*   Edges and Conditional Edges\n",
        "*   State\n",
        "*   Agents or Language Models\n",
        "\n"
      ],
      "metadata": {
        "id": "LwpA4Jg5937q"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "# Introdução ao Gemini"
      ],
      "metadata": {
        "id": "fhWGFuR348u3"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "O Gemini é um modelo de linguagem de grande escala (LLM) que foi desenvolvido para oferecer respostas mais precisas, rápidas e contextualmente relevantes. Ele é altamente eficiente em tarefas que envolvem processamento e geração de linguagem natural, como assistentes virtuais, respostas a consultas complexas e criação de conteúdo.\n",
        "\n",
        "O que diferencia o Gemini de outros modelos é sua capacidade de personalização e escalabilidade, permitindo que ele seja adaptado para resolver problemas específicos com uma performance superior. Isso faz dele uma escolha ideal para quem deseja criar soluções práticas e inovadoras com IA generativa."
      ],
      "metadata": {
        "id": "ygFM--xm6b7m"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Gemini com LangChain**"
      ],
      "metadata": {
        "id": "PhnGokxZ6imk"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Agora que já temos uma noção clara do potencial do Gemini, vamos ver como podemos integrá-lo com o LangChain. Essa integração permite que você combine o poder dos modelos de linguagem de larga escala com a flexibilidade do LangChain, resultando em agentes de IA altamente funcionais e interativos."
      ],
      "metadata": {
        "id": "qo2A7xIS6qLZ"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Configurando o Gemini**"
      ],
      "metadata": {
        "id": "OJ5ANy9162DC"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Para começar a trabalhar com o Gemini, vamos passar por algumas etapas de configuração, que incluem:\n",
        "\n",
        "**Criação da Conta e Obtenção das Credenciais de API:** Se você ainda não tem acesso ao Gemini, vamos ver como criar uma conta e obter as credenciais necessárias para utilizá-lo.\n"
      ],
      "metadata": {
        "id": "1Ylo0u-1685T"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "Acessar https://ai.google.dev/\n",
        "\n",
        "* Clique em \"Get API key in Google AI Studio\"\n",
        "* Acessar com uma conta do google\n",
        "* Clicar para gerar uma key para um novo projeto\n"
      ],
      "metadata": {
        "id": "cs1EgUwWlnNo"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Integração com o LangChain**: O próximo passo será conectar o Gemini ao LangChain, para que possamos utilizá-lo em nossos pipelines de IA. Vamos configurar o ambiente para garantir que o LangChain consiga se comunicar com o Gemini de forma eficaz.\n"
      ],
      "metadata": {
        "id": "SUDPocCx7hG0"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "import getpass\n",
        "import os"
      ],
      "metadata": {
        "id": "FYjIl_C57jtn"
      },
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "if \"GOOGLE_API_KEY\" not in os.environ:\n",
        "    os.environ[\"GOOGLE_API_KEY\"] = getpass.getpass(\"Enter your Google AI API key: \")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "D0pzCjTxmpOn",
        "outputId": "46413067-423c-4357-ef43-56791495364e"
      },
      "execution_count": 2,
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Enter your Google AI API key: ··········\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "Instalar o pacote langchain-google-genai"
      ],
      "metadata": {
        "id": "bazI17Ozmxtl"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "%pip install -qU langchain-google-genai"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EFT4XkITmuW1",
        "outputId": "323e4730-8d3e-4a62-deff-c07c9b506374"
      },
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.6/50.6 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m40.4/40.4 kB\u001b[0m \u001b[31m1.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m408.0/408.0 kB\u001b[0m \u001b[31m10.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m296.9/296.9 kB\u001b[0m \u001b[31m14.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m144.5/144.5 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m2.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_google_genai import ChatGoogleGenerativeAI\n",
        "\n",
        "llm = ChatGoogleGenerativeAI(\n",
        "    model=\"gemini-1.5-flash\",\n",
        "    temperature=0,\n",
        "    max_tokens=None,\n",
        "    timeout=None,\n",
        "    max_retries=2,\n",
        "    # other params...\n",
        ")"
      ],
      "metadata": {
        "id": "ldF5Bc3jnKMQ"
      },
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "**Primeiros Testes com o Gemini**: Com a configuração feita, vamos realizar alguns testes práticos para garantir que o Gemini esteja pronto para ser usado em nossos projetos. Isso nos dará uma noção inicial da capacidade do modelo e de como ele pode ser ajustado para atender a diferentes casos de uso."
      ],
      "metadata": {
        "id": "kONMx8rV7lZN"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "po9FFwbI7mjW"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}